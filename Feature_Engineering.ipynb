{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#FEATURE ENGINEERING\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7kYtDiUZlyQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Questions:"
      ],
      "metadata": {
        "id": "Nytn4Y-Smd79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.What is a parameter?\n",
        "\n",
        ".A parameter is a number describing a whole population (e.g., population mean), while a statistic is a number describing a sample (e.g., sample mean)."
      ],
      "metadata": {
        "id": "J1FRPnmumwZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.What is correlation?\n",
        "\n",
        ".Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate). It's a common tool for describing simple relationships without making a statement about cause and effect."
      ],
      "metadata": {
        "id": "jwo0RKg6nD1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3.What does negative correlation mean?\n",
        "\n",
        ".A negative correlation, also known as an inverse correlation, means that two variables tend to move in opposite directions, meaning as one variable increases, the other decreases, and vice versa."
      ],
      "metadata": {
        "id": "u_aiSABWnSpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4.Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        ".Machine learning is a field of artificial intelligence that enables systems to learn from data and improve performance without explicit programming. Its main components include algorithms, data, models, and predictions."
      ],
      "metadata": {
        "id": "18DzQCwNnjaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5.How does loss value help in determining whether the model is good or not?\n",
        "\n",
        ".Loss value helps determine model quality by quantifying the difference between predictions and actual values; a lower loss indicates better performance as the model's predictions are closer to the true values."
      ],
      "metadata": {
        "id": "ugJ-VlfOnzmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5.What are continuous and categorical variables?\n",
        "\n",
        ".In statistics, a continuous variable can take any value within a given range, while a categorical variable represents distinct categories or groups."
      ],
      "metadata": {
        "id": "wT-bWqKboFNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6How do we handle categorical variables in Machine Learning? What are the common t\n",
        "echniques?\n",
        "\n",
        ".To handle categorical variables in machine learning, you can use techniques like label encoding, one-hot encoding, ordinal encoding, and target encoding, depending on the nature of the categorical data and the model you're using.\n",
        "\n",
        "Common techniques across various fields include storytelling, emotional appeals, visual design, social proofing, repetition, and the use of humor, all aimed at effective communication and persuasion."
      ],
      "metadata": {
        "id": "Me-K8tk2pBs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.What do you mean by training and testing a dataset?\n",
        "\n",
        ".In machine learning, \"training\" a dataset means using a portion of the data to teach a model to recognize patterns and make predictions, while \"testing\" a dataset means using a separate portion to evaluate how well the trained model generalizes to unseen data."
      ],
      "metadata": {
        "id": "r_hSpbbVpZ4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8.What is sklearn.preprocessing?\n",
        "\n",
        ".In the context of scikit-learn (sklearn), \"preprocessing\" refers to the techniques used to transform raw data into a format more suitable for machine learning algorithms, including cleaning, scaling, and encoding data."
      ],
      "metadata": {
        "id": "X7c_-V3KpqMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9.What is a Test set?\n",
        "\n",
        ".In the context of machine learning and software testing, a \"test set\" is a subset of data used to evaluate the performance of a trained model or system, ensuring it generalizes well to unseen data."
      ],
      "metadata": {
        "id": "TfXzsRXcp9k8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10.How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        ".To split data for model fitting in Python, use train_test_split from scikit-learn, which randomly divides the data into training and testing sets, typically in an 80/20 or similar split.\n",
        "\n",
        "Prepare your data: Make sure your data is in a format suitable for scikit-learn (e.g., NumPy arrays or Pandas DataFrames).\n",
        "Separate features (X) and target (y): Identify the features you'll use to train your model and the target variable you're trying to predict.\n",
        "Use train_test_split:\n",
        "\n",
        "X and y are your feature and target arrays/DataFrames.\n",
        "test_size=0.2 specifies that 20% of the data should be used for testing (you can change this value).\n",
        "random_state=42 ensures reproducibility (you can change this value or omit it).\n",
        "Train your model: Use X_train and y_train to train your model.\n",
        "Evaluate your model: Use X_test and y_test to evaluate the performance of your trained model."
      ],
      "metadata": {
        "id": "uEX9MFqqqMGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11.Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        ".Performing Exploratory Data Analysis (EDA) before fitting a model is crucial because it helps you understand the data's structure, identify potential issues, and make informed decisions about model selection and feature engineering, ultimately leading to more accurate and reliable models."
      ],
      "metadata": {
        "id": "Pam1Pd39rDFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12.What is correlation?\n",
        "\n",
        ".Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate). It's a common tool for describing simple relationships without making a statement about cause and effect."
      ],
      "metadata": {
        "id": "4w7kLzTsrVvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13.What does negative correlation mean?\n",
        "\n",
        ".A negative correlation, also known as an inverse correlation, means that two variables tend to move in opposite directions, meaning as one variable increases, the other decreases, and vice vers"
      ],
      "metadata": {
        "id": "oouZ5Hsurqdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14.How can you find correlation between variables in Python?\n",
        "\n",
        ".To use the corrcoef() function, you need to pass in two arrays of data, one for each variable. The function will return a correlation matrix, which is a square matrix where the diagonal elements are always 1 and the off-diagonal elements indicate the correlations between different variables."
      ],
      "metadata": {
        "id": "JciJWXXbr6ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15.What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        ".Causation means one event directly leads to another, while correlation simply indicates a relationship between two events, where one doesn't necessarily cause the other.\n",
        "\n",
        "Theoretically, the difference between the two types of relationships are easy to identify â€” an action or occurrence can cause another (e.g. smoking causes an increase in the risk of developing lung cancer), or it can correlate with another (e.g. smoking is correlated with alcoholism, but it does not cause alcoholism).\n",
        "\n"
      ],
      "metadata": {
        "id": "UeXdgl7ssZM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        ".In the context of machine learning and deep learning, an optimizer is an algorithm that adjusts a model's parameters (like weights and biases) during training to minimize the loss function, aiming to improve model accuracy and performance.\n",
        "\n",
        "Here are some common types of optimizers, explained with examples:\n",
        "1. Stochastic Gradient Descent (SGD)\n",
        "Concept: SGD is a simple and widely used optimizer that updates the model's parameters based on the gradient of the loss function calculated on a small batch (or single data point) of training data.\n",
        "Example: Imagine training a model to predict house prices. SGD would randomly select a few houses from your dataset, calculate the error (loss) based on those houses' predicted prices, and then adjust the model's parameters to minimize that error.\n",
        "Pros: Simple to implement, computationally efficient.\n",
        "Cons: Can get stuck in local minima, slow convergence.\n",
        "2. Adam (Adaptive Moment Estimation)\n",
        "Concept:\n",
        "Adam combines the ideas of momentum and adaptive learning rates, making it a popular and often effective optimizer. It uses both the first and second moments of gradients to adaptively adjust the learning rate for each parameter.\n",
        "Example:\n",
        "In the house price prediction example, Adam would not only consider the current gradient (like SGD), but also the \"momentum\" of past gradients and adapt the learning rate for each parameter (e.g., the weight associated with square footage) based on how frequently that parameter has been updated.\n",
        "Pros:\n",
        "Often converges faster than SGD, adapts well to different problems.\n",
        "Cons:\n",
        "Can be computationally expensive, may require careful tuning of hyperparameters.\n",
        "3. RMSprop (Root Mean Square Propagation)\n",
        "Concept: RMSprop is another adaptive learning rate optimizer that uses the root mean square of the gradients to adapt the learning rate for each parameter.\n",
        "Example: In the house price prediction example, RMSprop would use the average of the squared gradients to determine how much to adjust the learning rate for each parameter.\n",
        "Pros: Effective for non-convex loss functions, can handle noisy gradients well.\n",
        "Cons: Can be sensitive to the learning rate hyperparameter.\n",
        "4. Adagrad (Adaptive Gradient Algorithm)\n",
        "Concept:\n",
        "Adagrad adapts the learning rate for each parameter based on the historical gradients, giving smaller learning rates to frequently updated parameters and larger learning rates to less frequently updated parameters.\n",
        "Example:\n",
        "In the house price prediction example, Adagrad would give smaller updates to parameters that are frequently updated (e.g., parameters related to common features like number of bedrooms) and larger updates to parameters that are less frequently updated (e.g., parameters related to less common features like a specific type of fireplace).\n",
        "Pros:\n",
        "Can be effective for sparse data, where some parameters are updated less frequently.\n",
        "Cons:\n",
        "Learning rate can decrease too rapidly, potentially leading to premature convergence.\n",
        "5. Momentum-based Gradient Descent\n",
        "Concept:\n",
        "Momentum-based gradient descent, often used in conjunction with SGD, adds a \"momentum\" term to the parameter updates, which helps the model to move faster in the direction of the minimum and escape local minima.\n",
        "Example:\n",
        "In the house price prediction example, momentum would allow the model to \"carry\" its speed from previous updates, helping it to quickly move towards the optimal parameter values.\n",
        "Pros:\n",
        "Can accelerate convergence, escape local minima more effectively.\n",
        "Cons:\n",
        "Can overshoot the minimum, requires careful tuning of the momentum hyperparameter."
      ],
      "metadata": {
        "id": "SSFW0GcCs7-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17.What is sklearn.linear_model ?\n",
        "\n",
        ".linear_model is a class of the sklearn module if contain different functions for performing machine learning with linear models. The term linear model implies that the model is specified as a linear combination of features.\n",
        "\n"
      ],
      "metadata": {
        "id": "d2-uX4TetjxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18.What does model.fit() do? What arguments must be given?\n",
        "\n",
        ".In TensorFlow,model. fit() function is used to train a machine learning model for a fixed number of epochs (iterations over the entire dataset). During training, the model adjusts its internal parameters (weights and biases) to minimize the loss function using optimization techniques like Gradient Descent.\n"
      ],
      "metadata": {
        "id": "WarXn0lyt29l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19.What does model.predict() do? What arguments must be given?\n",
        "\n",
        "\n",
        ".Model. predict passes the input vector through the model and returns the output tensor for each datapoint. Since the last layer in your model is a single Dense neuron, the output for any datapoint is a single value. And since you didn't specify an activation for the last layer, it will default to linear activation."
      ],
      "metadata": {
        "id": "-GKn-G3WuVKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20.What are continuous and categorical variables?\n",
        "\n",
        ".Continuous Variables:\n",
        "\n",
        "These variables are numerical and can take on any value within a specified range, including decimals and fractions.\n",
        "Examples include height, weight, temperature, or time.\n",
        "You can perform meaningful mathematical operations on continuous data.\n",
        "They are often measured using a measuring device.\n",
        "\n",
        "Categorical Variables:\n",
        "\n",
        "These variables represent categories or groups, rather than numerical values.\n",
        "Examples include gender (male/female), color (red/blue/green), or type of car (sedan/SUV/truck).\n",
        "You cannot perform arithmetic operations on categorical data.\n",
        "Categorical variables can be further categorized as nominal or ordinal.\n",
        "Nominal: Categories have no inherent order (e.g., colors, types of cars).\n",
        "Ordinal: Categories have a meaningful order or ranking (e.g., small, medium, large)."
      ],
      "metadata": {
        "id": "BWQugQh7uiO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21.What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Feature scaling is a data preprocessing technique that transforms numerical features to a common scale, ensuring they contribute equally to model training and preventing features with larger values from dominating the learning process.\n",
        "\n",
        "In machine learning, different features might have vastly different ranges or units (e.g., age vs. income). Without scaling, algorithms that rely on distance or gradient calculations (like k-nearest neighbors, support vector machines, and neural networks) can be heavily influenced by features with larger values, leading to biased or inaccurate models."
      ],
      "metadata": {
        "id": "x0tJ19eWu4ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22.How do we perform scaling in Python?\n",
        "\n",
        "Feature Scaling involves modifying values by one of two primary methods: Normalization or Standardization. Normalization takes the input values and modifies them to lie between 0 and 1. Standardization methods modify the values so that they center at 0 and have a standard deviation of 1."
      ],
      "metadata": {
        "id": "Sf0u-Zm1vgYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23.What is sklearn.preprocessing?\n",
        "\n",
        "In the context of scikit-learn (sklearn), \"preprocessing\" refers to the techniques used to transform raw data into a format more suitable for machine learning algorithms, including cleaning, scaling, and encoding data."
      ],
      "metadata": {
        "id": "Ondt5nBMvwyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24.How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        ".To split data into training and testing sets in Python, use the train_test_split function from scikit-learn, importing it as from sklearn.model_selection import train_test_split."
      ],
      "metadata": {
        "id": "yo8kH17uwA8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25.Explain data encoding?\n",
        "\n",
        ".Data encoding is the process of converting information into a specific format, often for storage, transmission, or processing by computers, ensuring efficient and reliable communication and manipulation of digital data.\n",
        "\n",
        "Here's a more detailed explanation:\n",
        "\n",
        "Computer Understanding:\n",
        "\n",
        "Computers primarily work with numerical data (binary code, 0s and 1s). Encoding allows us to represent various types of data (text, images, audio, etc.) in a format that computers can understand and process.\n",
        "\n",
        "Data Transmission:\n",
        "\n",
        "Encoding ensures that data can be transmitted reliably and efficiently across networks or stored in a way that can be easily retrieved.\n",
        "\n",
        "Data Security:\n",
        "\n",
        "While not the primary function of encoding, some encoding methods can contribute to data security by making data harder to understand without the proper decoding key.\n",
        "\n",
        "Machine Learning:\n",
        "\n",
        "In machine learning, data encoding is crucial for preparing data for algorithms that require numerical inputs, such as converting categorical data (e.g., colors, types) into numerical representations.\n",
        "\n",
        "Common Encoding Techniques:\n",
        "\n",
        "Character Encoding:\n",
        "\n",
        "ASCII: A standard for representing English characters using numbers (e.g., 'A' is 65).\n",
        "\n",
        "UTF-8: A more versatile encoding that supports a wider range of characters, including those used in many languages.\n",
        "\n",
        "Base64: An encoding scheme used to convert binary data into a text-based format, often used for transmitting data over channels that don't support binary data.\n",
        "\n",
        "Data Compression:\n",
        "\n",
        "Lossless Compression: Algorithms that compress data without losing any information (e.g., ZIP).\n",
        "\n",
        "Lossy Compression: Algorithms that compress data by discarding some information, which can lead to a reduction in quality (e.g., JPEG for images).\n",
        "\n",
        "Machine Learning Encoding:\n",
        "\n",
        "One-Hot Encoding: Converts categorical data into a binary matrix, where each category is represented by a unique column (0 or 1).\n",
        "\n",
        "Label Encoding: Assigns a unique numerical value to each category.\n",
        "\n",
        "Ordinal Encoding: Assigns numerical values based on the order or ranking of categories.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Text:\n",
        "\n",
        "When you type a word in a document, the characters are encoded using a character encoding like UTF-8 so that the computer can store and display them correctly.\n",
        "\n",
        "Images:\n",
        "\n",
        "Images are encoded using various formats like JPEG, PNG, or GIF, which employ encoding techniques to compress and represent the image data.\n",
        "\n",
        "Audio:\n",
        "\n",
        "Audio files are encoded using formats like MP3 or WAV, which use encoding to compress and represent the audio data.\n",
        "\n",
        "Machine Learning:\n",
        "\n",
        "When you have a dataset with categorical features like 'Gender' (Male/Female), you might use one-hot encoding to convert these categories into numerical representations for your machine learning model.\n",
        "Learn data encoding with online courses and programs - edX\n",
        "Data encoding is how we communicate information digitally and it's essential to sharing and storing information on computers.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3mYdKBcHwzU9"
      }
    }
  ]
}